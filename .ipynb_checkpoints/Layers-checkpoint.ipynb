{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Convolution:\n",
    "    #include init? idk\n",
    "        \n",
    "    # Given set of filters (features) #odd x #odd, iterate over input and multiply\n",
    "    # img_in: a square NxM #even x #even matrix representing one of the RGB values of the input\n",
    "    # features: filters/features that we compare the input against\n",
    "    # step_size: fixed to 1 (does not have any effect)\n",
    "    # returns a NxM matrix where each value is the result of multiplying\n",
    "    # filter with the input, then dividing by the size of the filter\n",
    "    # pad\n",
    "    def forward_prop(self, img_in, features, pad):\n",
    "        N = len(img_in)\n",
    "        M = len(img_in[0])\n",
    "        \n",
    "        # padded img as specified\n",
    "        img_in = self.padding(img_in, N + 2 * pad, M + 2 * pad)\n",
    "        # print(img_in)\n",
    "\n",
    "        # stride here is 1\n",
    "        # out_N = (N - features_size + 2*pad_N) / stride + 1\n",
    "        # out_M = (M - features_size + 2*pad_M) / stride + 1\n",
    "        stride = 1\n",
    "        # NxM output array (square)\n",
    "        # assume the number is divisible by the stride\n",
    "        out = np.zeros(( (N - len(features) + 2 * pad) // stride + 1\n",
    "                        , (M - len(features) + 2 * pad) // stride + 1 ))\n",
    "        \n",
    "        # iterate thru the input and output to corresponding part in out\n",
    "        for i in range(0, len(out)):\n",
    "            for j in range (0, len(out[0])):\n",
    "                # Take dot product of the flattened filter and part of input it is covering\n",
    "                out[i, j] = np.dot(np.ndarray.flatten(img_in[i:i+len(features),\n",
    "                            j:j+len(features[0])]), np.ndarray.flatten(features))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # assume stride = 1 (not tested for stride > 1)\n",
    "    # loss = loss gradient from the next layer (feeding back to previous layer)\n",
    "    # filter_weight = previous/original kernel/filter\n",
    "    # learning_rate = alpha or how much change we want (the same across the network)\n",
    "    # pad = pad_size must be the same as forward_prop's pad size for this layer.\n",
    "    # loss_size = img_size\n",
    "    def back_prop(self, img_in, loss, filter_weight, learning_rate, pad):\n",
    "        dL_dF = np.zeros(np.shape(filter_weight))\n",
    "        X = img_in\n",
    "        N = len(X)\n",
    "        M = len(X[0])\n",
    "\n",
    "        # padded img\n",
    "        X = self.padding(X, N + 2*pad, M + 2*pad)\n",
    "        print(\"padded img\", X)\n",
    "        \n",
    "        # update filter\n",
    "        # X is the image @ this stage\n",
    "        # chain rule: dL_dF = dL_dO * dO_dF\n",
    "        # but dL_dO is the input loss\n",
    "        # and dO_dF results in the image X\n",
    "        # dL_dF = convolution 2d between X (image @ this stage) and loss   \n",
    "        # ex. dL_dF 3x3 =  Conv image 4x4 (padded to 6x6), loss 4x4\n",
    "        for i in range(0, N - 1):\n",
    "            for j in range (0, M - 1):\n",
    "                # print(np.ndarray.flatten(X[i:i+len(loss), j:j+len(loss[0])]))\n",
    "                dL_dF[i, j] = np.dot(np.ndarray.flatten(X[i:i+len(loss),\n",
    "                            j:j+len(loss[0])]), np.ndarray.flatten(loss))\n",
    "        print(\"dL_dF\")\n",
    "        print(dL_dF)\n",
    "        \n",
    "        \n",
    "        # update the image X\n",
    "        # need to rotate the filter 180deg and pad it\n",
    "        # convolution from right to left and bottom to top\n",
    "        # F33 F32 F31\n",
    "        # F23 F22 F21\n",
    "        # F13 F12 F11\n",
    "        dL_dX = np.zeros(np.shape(img_in))\n",
    "        # rotate the filter 180deg\n",
    "        rot_filter_weight = np.rot90(filter_weight, 2)\n",
    "        # pad the filter \n",
    "        loss = self.padding(loss, N + 2*pad, M + 2*pad)\n",
    "        print(\"padded loss\")\n",
    "        print(loss)\n",
    "        # dL_dX = convolution 2d between rot180(Filter) and loss \n",
    "        # convolution from right to left and bottom to top\n",
    "        for i in range(N - 1, -1, -1):\n",
    "            for j in range (M - 1, -1, -1):\n",
    "                #print(i,j)\n",
    "                #print(np.ndarray.flatten(loss[i:i+len(rot_filter_weight), j:j+len(rot_filter_weight[0])]))\n",
    "                dL_dX[i, j] = np.dot(np.ndarray.flatten(loss[i:i+len(rot_filter_weight),\n",
    "                            j:j+len(rot_filter_weight[0])]), np.ndarray.flatten(rot_filter_weight))\n",
    "        \n",
    "        print(\"dL_dX\")\n",
    "        print(dL_dX)\n",
    "        # F = F - a * dL/dF\n",
    "        filter_weight = filter_weight - learning_rate * dL_dF\n",
    "        print(\"updated_features\")\n",
    "        print(filter_weight)\n",
    "        return filter_weight, X\n",
    "    \n",
    "    # Add 0s to the border of orig_img as needed to achieve NxM matrix\n",
    "    # assume orig_img is a square matrix with #odd x #odd\n",
    "    def padding(self, orig_img, N, M):\n",
    "        if N - orig_img.shape[0] < 0 or M - orig_img.shape[1] < 0:\n",
    "            return \"target NxM must be larger or equal to orig_img\"\n",
    "        starting_row = (N - orig_img.shape[0]) // 2\n",
    "        starting_column = (M - orig_img.shape[1]) // 2\n",
    "        pad_arr = np.zeros((N, M))\n",
    "        pad_arr[starting_row:starting_row+orig_img.shape[0], starting_column:starting_column+orig_img.shape[1]] = orig_img\n",
    "        return pad_arr\n",
    "    \n",
    "# Can implement either max or avg pooling, going w max for now\n",
    "class Pooling:\n",
    "    \n",
    "    def __init__(self, img_in, pool_dim):\n",
    "        # input data to pooling\n",
    "        self.img_in = img_in\n",
    "        # need to remember which indices provide the local max for back_prop\n",
    "        self.mask = np.zeros((len(img_in),len(img_in[0])))\n",
    "        # pooling dimension for this layer\n",
    "        self.pool_dim = pool_dim\n",
    "        # output data from pooling\n",
    "        # reduce by a factor of whatever stride is\n",
    "        self.out = np.zeros((len(img_in) // self.pool_dim, len(img_in[0]) // self.pool_dim))\n",
    "    \n",
    "    # Go thru sections of the original matrix and only take the highest value\n",
    "    # Put this into a new PxQ matrix\n",
    "    # Pool dim is a single int representing both size of the 'pool filter' and\n",
    "    # the stride. Ex: 2 -> 2x2 pooling with stride 2\n",
    "    def forward_prop(self):\n",
    "        pool_dim = self.pool_dim\n",
    "        # loop thru input matrix\n",
    "        for i in range(0, len(self.out)):\n",
    "            for j in range(0, len(self.out[0])):\n",
    "                # get largest value in pool\n",
    "                vert = i * pool_dim\n",
    "                horiz = j * pool_dim\n",
    "                pool = self.img_in[vert:vert+pool_dim, horiz:horiz+pool_dim]\n",
    "                #print(pool)\n",
    "                self.out[i,j] = np.amax(pool)\n",
    "                max_idx = np.argmax(pool)\n",
    "                # flag local max indices\n",
    "                self.mask[vert + max_idx // pool_dim, horiz + max_idx % pool_dim] = 1\n",
    "              \n",
    "        return self.out, self.mask\n",
    "    \n",
    "    # only send loss to those local max's, the rest is 0\n",
    "    def back_prop(self, loss):\n",
    "        # cheap fix to get the 1d index\n",
    "        loss_flattened = np.ndarray.flatten(loss)\n",
    "        count = 0\n",
    "        back_loss = np.zeros((len(self.mask),len(self.mask[0])))\n",
    "        \n",
    "        for i in range(0, len(self.mask)):\n",
    "            for j in range(0, len(self.mask[0])):\n",
    "                if self.mask[i,j] == 1:\n",
    "                    back_loss[i,j] = loss_flattened[count]\n",
    "                    count = count + 1\n",
    "                else:\n",
    "                    back_loss[i,j] = 0\n",
    "        # zero out mask for the next iteration\n",
    "        # self.mask = np.zeros((len(img_in),len(img_in[0])))\n",
    "        return back_loss\n",
    "    \n",
    "    def padding(self, orig_img, N, M):\n",
    "        starting_row = int((N - orig_img.shape[0]) // 2)\n",
    "        starting_column = int((M - orig_img.shape[1]) // 2)\n",
    "        pad_arr = np.zeros((N, M))\n",
    "        pad_arr[starting_row:starting_row+orig_img.shape[0], starting_column:starting_column+orig_img.shape[1]] = orig_img\n",
    "        return pad_arr\n",
    "    \n",
    "class ReLU():\n",
    "    \n",
    "    def __init__(self, img_in):\n",
    "        self.img_in = img_in\n",
    "        \n",
    "    def forward_prop(self):\n",
    "        for i in range(0, len(self.img_in)):\n",
    "            for j in range(0, len(self.img_in[0])):\n",
    "                if self.img_in[i, j] < 0:\n",
    "                    self.img_in[i, j] = 0\n",
    "        return self.img_in\n",
    "    \n",
    "    # send loss to those location with input > 0, the rest is 0\n",
    "    def back_prop(self, loss):\n",
    "        back_loss = np.zeros(np.shape(self.img_in))\n",
    "        for i in range(0, len(self.img_in)):\n",
    "            for j in range(0, len(self.img_in[0])):\n",
    "                # print(self.img_in[i, j])\n",
    "                if self.img_in[i, j] <= 0:\n",
    "                    back_loss[i,j] = 0\n",
    "                else:\n",
    "                    back_loss[i,j] = loss[i,j]\n",
    "        print(\"back_loss\")\n",
    "        print(back_loss)\n",
    "        return back_loss\n",
    "    \n",
    "class Sigmoid_Act():\n",
    "    def forward_prop():\n",
    "        return None\n",
    "    def back_prop():\n",
    "        return None\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: conv_forward\n",
      "[[ 5. 17. 24. 16.]\n",
      " [ 0. 25.  6. 12.]\n",
      " [ 3. 23. 40. 45.]\n",
      " [-2. 12. 16. -2.]]\n",
      "TEST: pool_forward\n",
      "(array([[6., 8.],\n",
      "       [5., 4.]]), array([[0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 1., 0.],\n",
      "       [0., 0., 0., 0.]]))\n",
      "(array([[6., 8.],\n",
      "       [5., 4.]]), array([[0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 1., 0.],\n",
      "       [0., 0., 0., 0.]]))\n",
      "TEST: ReLU\n",
      "[[5 0 0]\n",
      " [0 6 2]\n",
      " [5 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "def test_conv_forward():\n",
    "    test_arr = np.array(([5, 5, 2, 7],\n",
    "                         [5, 5, 5, 0],\n",
    "                         [5, 5, 5, 22],\n",
    "                         [1, 2, 3, 4]))\n",
    "    feat_arr = np.array(([2, -1, 0], \n",
    "                         [2, 1, 1],\n",
    "                         [1, 0, -1]))\n",
    "    conv_layer = Convolution()\n",
    "    print(conv_layer.forward_prop(test_arr, feat_arr,1))\n",
    "    \n",
    "def test_pool_forward():\n",
    "    test_arr = np.array(([5, 5, 2, 8],\n",
    "                         [5, 6, 2, 1],\n",
    "                         [5, 5, 4, -4],\n",
    "                         [1, 4, 2, 3]))\n",
    "    pool_dim = 2\n",
    "    pool_layer = Pooling(test_arr, pool_dim)\n",
    "    print(pool_layer.forward_prop())\n",
    "    pool_layer = Pooling(test_arr, pool_dim)\n",
    "    pool_dim = 1\n",
    "    print(pool_layer.forward_prop())\n",
    "    \n",
    "def test_ReLU():\n",
    "    test_arr = np.array(([5, -1, -1],\n",
    "                         [-4, 6, 2],\n",
    "                         [5, 0, -3333]))\n",
    "    ReLU_layer = ReLU(test_arr)\n",
    "    print(ReLU_layer.forward_prop())\n",
    "    \n",
    "print(\"TEST: conv_forward\")    \n",
    "test_conv_forward()\n",
    "print(\"TEST: pool_forward\")\n",
    "test_pool_forward()\n",
    "print(\"TEST: ReLU\")\n",
    "test_ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: padding\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  5.  5.  2.  7.  0.]\n",
      " [ 0.  5.  5.  5.  0.  0.]\n",
      " [ 0.  5.  5.  5. 22.  0.]\n",
      " [ 0.  1.  2.  3.  4.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  5.  5.  2.  7.  0.  0.]\n",
      " [ 0.  0.  5.  5.  5.  0.  0.  0.]\n",
      " [ 0.  0.  5.  5.  5. 22.  0.  0.]\n",
      " [ 0.  0.  1.  2.  3.  4.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 2. 3. 0.]\n",
      " [0. 4. 5. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[2. 3.]\n",
      " [4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "# tests 2\n",
    "# def padding(self, orig_img, N, M)\n",
    "def test_padding():\n",
    "    conv_layer = Convolution()\n",
    "    test_arr = np.array(([5, 5, 2, 7],\n",
    "                         [5, 5, 5, 0],\n",
    "                         [5, 5, 5, 22],\n",
    "                         [1, 2, 3, 4]))\n",
    "    print(conv_layer.padding(test_arr,6,6))\n",
    "    test_arr = np.array(([5, 5, 2, 7],\n",
    "                         [5, 5, 5, 0],\n",
    "                         [5, 5, 5, 22],\n",
    "                         [1, 2, 3, 4]))\n",
    "    print(conv_layer.padding(test_arr,8,8))\n",
    "    test_arr = np.array(([2, 3],\n",
    "                         [4, 5]))\n",
    "    print(conv_layer.padding(test_arr,4,4))\n",
    "    \n",
    "    test_arr = np.array(([2, 3],\n",
    "                         [4, 5]))\n",
    "    print(conv_layer.padding(test_arr,2,2))\n",
    "    \n",
    "print(\"TEST: padding\")\n",
    "test_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: conv_back_prop\n",
      "padded img [[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  5.  5.  2.  7.  0.]\n",
      " [ 0.  5.  5.  5.  0.  0.]\n",
      " [ 0.  5.  5.  5. 22.  0.]\n",
      " [ 0.  4.  3.  2.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "dL_dF\n",
      "[[ 48.  22.  59.]\n",
      " [103. 184. 248.]\n",
      " [147. 147. 248.]]\n",
      "padded loss\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  5.  3.  1.  9.  0.]\n",
      " [ 0.  2.  4.  6. -1.  0.]\n",
      " [ 0.  9.  7.  5. -3.  0.]\n",
      " [ 0. -1. -2. -3. -4.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "dL_dX\n",
      "[[ 17.  18.  14.  11.]\n",
      " [ 18.  17.   3.   7.]\n",
      " [ 24.  26.  -4.   0.]\n",
      " [  2. -13. -23. -12.]]\n",
      "updated_features\n",
      "[[ 1.52 -1.22 -0.59]\n",
      " [ 0.97 -0.84 -1.48]\n",
      " [-0.47 -1.47 -3.48]]\n"
     ]
    }
   ],
   "source": [
    "# tests 3\n",
    "# def back_prop(self, img_in, loss, features, learning_rate)\n",
    "def test_conv_backward():\n",
    "    test_arr = np.array(([5, 5, 2, 7],\n",
    "                         [5, 5, 5, 0],\n",
    "                         [5, 5, 5, 22],\n",
    "                         [4, 3, 2, 1]))\n",
    "    feat_arr = np.array(([2, -1, 0], \n",
    "                         [2, 1, 1],\n",
    "                         [1, 0, -1]))\n",
    "    loss = np.array(([5, 3, 1, 9], \n",
    "                     [2, 4, 6, -1],\n",
    "                     [9, 7, 5, -3],\n",
    "                     [-1,-2,-3,-4]))\n",
    "    conv_layer = Convolution()\n",
    "    conv_layer.back_prop(test_arr, loss, feat_arr, 0.01, 1)\n",
    "    \n",
    "print(\"TEST: conv_back_prop\")\n",
    "test_conv_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: pool_back_prop\n",
      "(array([[6., 8.],\n",
      "       [5., 4.]]), array([[0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 1., 0.],\n",
      "       [0., 0., 0., 0.]]))\n",
      "[[ 0.  0.  0.  9.]\n",
      " [ 0.  4.  0.  0.]\n",
      " [ 1.  0. -8.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# def back_prop(self, loss):\n",
    "def test_pooling_backward():\n",
    "    test_arr = np.array(([5, 5, 2, 8],\n",
    "                             [5, 6, 2, 1],\n",
    "                             [5, 5, 4, -4],\n",
    "                             [1, 4, 2, 3]))\n",
    "    test_loss = np.array(([9,4],[1,-8]))\n",
    "    pool_dim = 2\n",
    "    pool_layer = Pooling(test_arr, pool_dim)\n",
    "    print(pool_layer.forward_prop())\n",
    "    print(pool_layer.back_prop(test_loss))\n",
    "    \n",
    "print(\"TEST: pool_back_prop\")\n",
    "test_pooling_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: relu_back_prop\n",
      "back_loss\n",
      "[[1. 0. 0.]\n",
      " [0. 5. 6.]\n",
      " [7. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# def back_prop(self, loss):\n",
    "def test_ReLU_backward():\n",
    "    test_arr = np.array(([5, -1, -1],\n",
    "                         [-4, 6, 2],\n",
    "                         [5, 0, -3333]))\n",
    "    ReLU_layer = ReLU(test_arr)\n",
    "    ReLU_layer.forward_prop()\n",
    "    test_loss = np.array(([1, 2, 3],\n",
    "                         [4, 5, 6],\n",
    "                         [7, 8, 9]))\n",
    "    ReLU_layer.back_prop(test_loss)\n",
    "\n",
    "print(\"TEST: relu_back_prop\")\n",
    "test_ReLU_backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
