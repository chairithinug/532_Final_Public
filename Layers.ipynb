{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Convolution:\n",
    "    #include init? idk\n",
    "    # num_img_in: Number of inputs\n",
    "    # filter_size: Size of kernel, will be square & less than 28\n",
    "    # num_filters: Number of filters to run input against\n",
    "    def __init__(self, num_img_in, filter_size, num_filters, step_size):\n",
    "        self.num_img_in = num_img_in\n",
    "        self.filter_size = filter_size\n",
    "        self.num_filters = num_filters\n",
    "        self.step_size = step_size\n",
    "        \n",
    "    def forward_prop(self, img_in, features):\n",
    "        N = len(img_in)\n",
    "        M = len(img_in[0])\n",
    "        step_size = self.step_size\n",
    "        # if step size doesn't match up -> padding()\n",
    "        # gives us amount of padding needed\\n\",\n",
    "        pad_N = (N - len(features)) % step_size\n",
    "        pad_M = (M - len(features[0])) % step_size\n",
    "        img_in = self.padding(img_in, N+pad_N, M+pad_M) #padded img\n",
    "\n",
    "        # should be whole numbers\\n\",\n",
    "        P = (len(img_in) - len(features)) / step_size + 1\n",
    "        Q = (len(img_in[0]) - len(features)) / step_size + 1\n",
    "\n",
    "        # find good ways to add padding, possibly this part is not needed?\n",
    "        if P.is_integer() is False:\n",
    "            return \"Bad Dimension P\"\n",
    "        if Q.is_integer() is False:\n",
    "            return \"Bad dimension Q\"\n",
    "        else:\n",
    "            P = int(P)\n",
    "            Q = int(Q)\n",
    "\n",
    "        print(P, Q)\n",
    "        out = np.zeros((self.num_filters, P, Q)) # PxQ output array\\n\",\n",
    "\n",
    "        # iterate thru the input and output to corresponding part in out\n",
    "        # output f amount of filters from each layer\n",
    "        for f in range(0, self.num_filters):\n",
    "            for i in range(0, P):\n",
    "                for j in range (0, Q):\n",
    "                    # Take dot product of the flattened filter and part it is covering\\n\",\n",
    "                    out[f, i, j] = np.dot(np.ndarray.flatten(img_in[i:i+len(features),\n",
    "                        j:j+len(features[0])]), np.ndarray.flatten(features))\n",
    "        return out\n",
    "        \n",
    "    # Given set of filters (features) #odd x #odd, iterate over input and multiply\n",
    "    # img_in: a square NxM #even x #even matrix representing one of the RGB values of the input\n",
    "    # features: filters/features that we compare the input against\n",
    "    # step_size: fixed to 1 (does not have any effect)\n",
    "    # returns a NxM matrix where each value is the result of multiplying\n",
    "    # filter with the input, then dividing by the size of the filter\n",
    "    # pad\n",
    "    # Need to add filters to feature map\n",
    "    def forward_prop_A(self, img_in, features, pad):\n",
    "        N = len(img_in)\n",
    "        M = len(img_in[0])\n",
    "        \n",
    "        # padded img as specified\n",
    "        img_in = self.padding(img_in, N + 2 * pad, M + 2 * pad)\n",
    "        plt.imshow(features)\n",
    "        plt.show()\n",
    "        plt.imshow(img_in)\n",
    "        plt.show()\n",
    "        # print(img_in)\n",
    "\n",
    "        # stride here is 1\n",
    "        # out_N = (N - features_size + 2*pad_N) / stride + 1\n",
    "        # out_M = (M - features_size + 2*pad_M) / stride + 1\n",
    "        stride = 1\n",
    "        # NxM output array (square)\n",
    "        # assume the number is divisible by the stride\n",
    "        out = np.zeros(((N - len(features) + 2 * pad) // stride + 1\n",
    "                        , (M - len(features[0]) + 2 * pad) // stride + 1 ))\n",
    "        \n",
    "        # iterate thru the input and output to corresponding part in out\n",
    "        #for f in range(0, self.num_filters):\n",
    "        for i in range(0, len(out)):\n",
    "            for j in range (0, len(out[0])):\n",
    "                # Take dot product of the flattened filter and part of input it is covering\n",
    "                out[i, j] = np.dot(np.ndarray.flatten(img_in[i:i+len(features),\n",
    "                                j:j+len(features[0])]), np.ndarray.flatten(features))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # assume stride = 1 (not tested for stride > 1)\n",
    "    # loss = loss gradient from the next layer (feeding back to previous layer)\n",
    "    # filter_weight = previous/original kernel/filter\n",
    "    # learning_rate = alpha or how much change we want (the same across the network)\n",
    "    # pad = pad_size must be the same as forward_prop's pad size for this layer.\n",
    "    # loss_size = img_size\n",
    "    def back_prop(self, img_in, loss, filter_weight, learning_rate, pad):\n",
    "        dL_dF = np.zeros(np.shape(filter_weight))\n",
    "        X = img_in\n",
    "        N = len(X)\n",
    "        M = len(X[0])\n",
    "\n",
    "        # padded img\n",
    "        X = self.padding(X, N + 2*pad, M + 2*pad)\n",
    "        print(\"padded img\", X)\n",
    "        \n",
    "        # update filter\n",
    "        # X is the image @ this stage\n",
    "        # chain rule: dL_dF = dL_dO * dO_dF\n",
    "        # but dL_dO is the input loss\n",
    "        # and dO_dF results in the image X\n",
    "        # dL_dF = convolution 2d between X (image @ this stage) and loss   \n",
    "        # ex. dL_dF 3x3 =  Conv image 4x4 (padded to 6x6), loss 4x4\n",
    "        for i in range(0, N - 1):\n",
    "            for j in range (0, M - 1):\n",
    "                # print(np.ndarray.flatten(X[i:i+len(loss), j:j+len(loss[0])]))\n",
    "                dL_dF[i, j] = np.dot(np.ndarray.flatten(X[i:i+len(loss),\n",
    "                            j:j+len(loss[0])]), np.ndarray.flatten(loss))\n",
    "        print(\"dL_dF\")\n",
    "        print(dL_dF)\n",
    "        \n",
    "        \n",
    "        # update the image X\n",
    "        # need to rotate the filter 180deg and pad it\n",
    "        # convolution from right to left and bottom to top\n",
    "        # F33 F32 F31\n",
    "        # F23 F22 F21\n",
    "        # F13 F12 F11\n",
    "        dL_dX = np.zeros(np.shape(img_in))\n",
    "        # rotate the filter 180deg\n",
    "        rot_filter_weight = np.rot90(filter_weight, 2)\n",
    "        # pad the filter \n",
    "        loss = self.padding(loss, N + 2*pad, M + 2*pad)\n",
    "        print(\"padded loss\")\n",
    "        print(loss)\n",
    "        # dL_dX = convolution 2d between rot180(Filter) and loss \n",
    "        # convolution from right to left and bottom to top\n",
    "        for i in range(N - 1, -1, -1):\n",
    "            for j in range (M - 1, -1, -1):\n",
    "                #print(i,j)\n",
    "                #print(np.ndarray.flatten(loss[i:i+len(rot_filter_weight), j:j+len(rot_filter_weight[0])]))\n",
    "                dL_dX[i, j] = np.dot(np.ndarray.flatten(loss[i:i+len(rot_filter_weight),\n",
    "                            j:j+len(rot_filter_weight[0])]), np.ndarray.flatten(rot_filter_weight))\n",
    "        \n",
    "        print(\"dL_dX\")\n",
    "        print(dL_dX)\n",
    "        # F = F - a * dL/dF\n",
    "        filter_weight = filter_weight - learning_rate * dL_dF\n",
    "        print(\"updated_features\")\n",
    "        print(filter_weight)\n",
    "        return filter_weight, X\n",
    "    \n",
    "    # Add 0s to the border of orig_img as needed to achieve NxM matrix\n",
    "    # assume orig_img is a square matrix with #odd x #odd\n",
    "    def padding(self, orig_img, N, M):\n",
    "        if N - orig_img.shape[0] < 0 or M - orig_img.shape[1] < 0:\n",
    "            raise \"target NxM must be larger or equal to orig_img\"\n",
    "        starting_row = (N - orig_img.shape[0]) // 2\n",
    "        starting_column = (M - orig_img.shape[1]) // 2\n",
    "        pad_arr = np.zeros((N, M))\n",
    "        pad_arr[starting_row:starting_row+orig_img.shape[0], starting_column:starting_column+orig_img.shape[1]] = orig_img\n",
    "        return pad_arr\n",
    "    \n",
    "# Can implement either max or avg pooling, going w max for now\n",
    "class Pooling:\n",
    "    \n",
    "    def __init__(self, pool_dim):\n",
    "        # input data to pooling\n",
    "        #self.img_in = img_in\n",
    "        # need to remember which indices provide the local max for back_prop\n",
    "        # self.mask = np.zeros((len(img_in),len(img_in[0])))\n",
    "        # pooling dimension for this layer\n",
    "        self.pool_dim = pool_dim\n",
    "        # output data from pooling\n",
    "        # reduce by a factor of whatever stride is\n",
    "        #self.out = np.zeros((len(img_in) // self.pool_dim, len(img_in[0]) // self.pool_dim))\n",
    "    \n",
    "    # Go thru sections of the original matrix and only take the highest value\n",
    "    # Put this into a new PxQ matrix\n",
    "    # Pool dim is a single int representing both size of the 'pool filter' and\n",
    "    # the stride. Ex: 2 -> 2x2 pooling with stride 2\n",
    "    def forward_prop(self, img_in):\n",
    "        # Input known at runtime, not while adding layers\n",
    "        self.img_in = img_in\n",
    "        self.out = np.zeros((len(img_in) // self.pool_dim, len(img_in[0]) // self.pool_dim))\n",
    "        self.mask = np.zeros((len(img_in),len(img_in[0])))\n",
    "        \n",
    "        pool_dim = self.pool_dim\n",
    "        # loop thru input matrix\n",
    "        for i in range(0, len(self.out)):\n",
    "            for j in range(0, len(self.out[0])):\n",
    "                # get largest value in pool\n",
    "                vert = i * pool_dim\n",
    "                horiz = j * pool_dim\n",
    "                pool = self.img_in[vert:vert+pool_dim, horiz:horiz+pool_dim]\n",
    "                #print(pool)\n",
    "                self.out[i,j] = np.amax(pool)\n",
    "                max_idx = np.argmax(pool)\n",
    "                # flag local max indices\n",
    "                self.mask[vert + max_idx // pool_dim, horiz + max_idx % pool_dim] = 1.0\n",
    "              \n",
    "        return self.out, self.mask\n",
    "    \n",
    "    # only send loss to those local max's, the rest is 0\n",
    "    def back_prop(self, loss):\n",
    "        # cheap fix to get the 1d index\n",
    "        loss_flattened = np.ndarray.flatten(loss)\n",
    "        count = 0\n",
    "        back_loss = np.zeros((len(self.mask),len(self.mask[0])))\n",
    "        \n",
    "        for i in range(0, len(self.mask)):\n",
    "            for j in range(0, len(self.mask[0])):\n",
    "                if self.mask[i,j] == 1:\n",
    "                    back_loss[i,j] = loss_flattened[count]\n",
    "                    count = count + 1\n",
    "                else:\n",
    "                    back_loss[i,j] = 0\n",
    "        # zero out mask for the next iteration\n",
    "        # self.mask = np.zeros((len(img_in),len(img_in[0])))\n",
    "        return back_loss\n",
    "    \n",
    "    def padding(self, orig_img, N, M):\n",
    "        starting_row = int((N - orig_img.shape[0]) // 2)\n",
    "        starting_column = int((M - orig_img.shape[1]) // 2)\n",
    "        pad_arr = np.zeros((N, M))\n",
    "        pad_arr[starting_row:starting_row+orig_img.shape[0], starting_column:starting_column+orig_img.shape[1]] = orig_img\n",
    "        return pad_arr\n",
    "    \n",
    "class ReLU():\n",
    "    \n",
    "    def __init__(self):\n",
    "        #self.img_in = img_in\n",
    "        pass\n",
    "        \n",
    "    def forward_prop(self, img_in):\n",
    "        self.img_in = img_in\n",
    "        for i in range(0, len(self.img_in)):\n",
    "            for j in range(0, len(self.img_in[0])):\n",
    "                if self.img_in[i, j] < 0:\n",
    "                    self.img_in[i, j] = 0.0\n",
    "        return self.img_in\n",
    "    \n",
    "    # send loss to those location with input > 0, the rest is 0\n",
    "    def back_prop(self, loss):\n",
    "        back_loss = np.zeros(np.shape(self.img_in))\n",
    "        for i in range(0, len(self.img_in)):\n",
    "            for j in range(0, len(self.img_in[0])):\n",
    "                # print(self.img_in[i, j])\n",
    "                if self.img_in[i, j] <= 0:\n",
    "                    back_loss[i,j] = 0\n",
    "                else:\n",
    "                    back_loss[i,j] = loss[i,j]\n",
    "        print(\"back_loss\")\n",
    "        print(back_loss)\n",
    "        return back_loss\n",
    "    \n",
    "    \n",
    "## COPIED ##\n",
    "class FullyConnected:\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, learning_rate):\n",
    "        self.weights = 0.01*np.random.rand(num_inputs, num_outputs)\n",
    "        self.bias = np.zeros((num_outputs, 1))\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.dot(self.inputs, self.weights) + self.bias.T\n",
    "\n",
    "    def backward(self, dy):\n",
    "\n",
    "        if dy.shape[0] == self.inputs.shape[0]:\n",
    "            dy = dy.T\n",
    "        dw = dy.dot(self.inputs)\n",
    "        db = np.sum(dy, axis=1, keepdims=True)\n",
    "        dx = np.dot(dy.T, self.weights.T)\n",
    "\n",
    "        self.weights -= self.lr * dw.T\n",
    "        self.bias -= self.lr * db\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def extract(self):\n",
    "        return {self.name+'.weights':self.weights, self.name+'.bias':self.bias}\n",
    "\n",
    "    def feed(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "        \n",
    "## COPIED ##\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, inputs):\n",
    "        exp = np.exp(inputs, dtype=np.float)\n",
    "        self.out = exp/np.sum(exp)\n",
    "        return self.out\n",
    "    def backward(self, dy):\n",
    "        return self.out.T - dy.reshape(dy.shape[0],1)\n",
    "    def extract(self):\n",
    "        return\n",
    "    \n",
    "class Sigmoid_Act():\n",
    "    def forward_prop():\n",
    "        return None\n",
    "    def back_prop():\n",
    "        return None\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: conv_forward\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "TEST: pool_forward\n",
      "(array([[ 6.,  8.],\n",
      "       [ 5.,  4.]]), array([[ 0.,  0.,  0.,  1.],\n",
      "       [ 0.,  1.,  0.,  0.],\n",
      "       [ 1.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.]]))\n",
      "(array([[ 6.,  8.],\n",
      "       [ 5.,  4.]]), array([[ 0.,  0.,  0.,  1.],\n",
      "       [ 0.,  1.,  0.,  0.],\n",
      "       [ 1.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.]]))\n",
      "TEST: ReLU\n",
      "[[5 0 0]\n",
      " [0 6 2]\n",
      " [5 0 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # tests\n",
    "# def test_conv_forward():\n",
    "#     test_arr = np.array(([5, 5, 2, 7],\n",
    "#                          [5, 5, 5, 0],\n",
    "#                          [5, 5, 5, 22],\n",
    "#                          [1, 2, 3, 4]))\n",
    "#     feat_arr = np.array(([2, -1, 0], \n",
    "#                          [2, 1, 1],\n",
    "#                          [1, 0, -1]))\n",
    "#     plt.imshow(test_arr)\n",
    "#     plt.show()\n",
    "#     conv_layer = Convolution(1, 3, 1)\n",
    "#     plt.imshow(conv_layer.forward_prop(test_arr, feat_arr,1))\n",
    "    \n",
    "# def test_pool_forward():\n",
    "#     test_arr = np.array(([5, 5, 2, 8],\n",
    "#                          [5, 6, 2, 1],\n",
    "#                          [5, 5, 4, -4],\n",
    "#                          [1, 4, 2, 3]))\n",
    "#     pool_dim = 2\n",
    "#     pool_layer = Pooling(pool_dim)\n",
    "#     print(pool_layer.forward_prop(test_arr))\n",
    "#     pool_layer = Pooling(pool_dim)\n",
    "#     pool_dim = 1\n",
    "#     print(pool_layer.forward_prop(test_arr))\n",
    "    \n",
    "# def test_ReLU():\n",
    "#     test_arr = np.array(([5, -1, -1],\n",
    "#                          [-4, 6, 2],\n",
    "#                          [5, 0, -3333]))\n",
    "#     ReLU_layer = ReLU()\n",
    "#     print(ReLU_layer.forward_prop(test_arr))\n",
    "    \n",
    "# print(\"TEST: conv_forward\")    \n",
    "# test_conv_forward()\n",
    "# print(\"TEST: pool_forward\")\n",
    "# test_pool_forward()\n",
    "# print(\"TEST: ReLU\")\n",
    "# test_ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: padding\n",
      "[[  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   5.   5.   2.   7.   0.]\n",
      " [  0.   5.   5.   5.   0.   0.]\n",
      " [  0.   5.   5.   5.  22.   0.]\n",
      " [  0.   1.   2.   3.   4.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]]\n",
      "[[  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   5.   5.   2.   7.   0.   0.]\n",
      " [  0.   0.   5.   5.   5.   0.   0.   0.]\n",
      " [  0.   0.   5.   5.   5.  22.   0.   0.]\n",
      " [  0.   0.   1.   2.   3.   4.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  2.  3.  0.]\n",
      " [ 0.  4.  5.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[[ 2.  3.]\n",
      " [ 4.  5.]]\n"
     ]
    }
   ],
   "source": [
    "# # tests 2\n",
    "# # def padding(self, orig_img, N, M)\n",
    "# def test_padding():\n",
    "#     conv_layer = Convolution(1, 6, 6)\n",
    "#     test_arr = np.array(([5, 5, 2, 7],\n",
    "#                          [5, 5, 5, 0],\n",
    "#                          [5, 5, 5, 22],\n",
    "#                          [1, 2, 3, 4]))\n",
    "#     print(conv_layer.padding(test_arr,6,6))\n",
    "#     test_arr = np.array(([5, 5, 2, 7],\n",
    "#                          [5, 5, 5, 0],\n",
    "#                          [5, 5, 5, 22],\n",
    "#                          [1, 2, 3, 4]))\n",
    "#     print(conv_layer.padding(test_arr,8,8))\n",
    "#     test_arr = np.array(([2, 3],\n",
    "#                          [4, 5]))\n",
    "#     print(conv_layer.padding(test_arr,4,4))\n",
    "    \n",
    "#     test_arr = np.array(([2, 3],\n",
    "#                          [4, 5]))\n",
    "#     print(conv_layer.padding(test_arr,2,2))\n",
    "    \n",
    "# print(\"TEST: padding\")\n",
    "# test_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: conv_back_prop\n",
      "padded img [[  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   5.   5.   2.   7.   0.]\n",
      " [  0.   5.   5.   5.   0.   0.]\n",
      " [  0.   5.   5.   5.  22.   0.]\n",
      " [  0.   4.   3.   2.   1.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]]\n",
      "dL_dF\n",
      "[[  48.   22.   59.]\n",
      " [ 103.  184.  248.]\n",
      " [ 147.  147.  248.]]\n",
      "padded loss\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  5.  3.  1.  9.  0.]\n",
      " [ 0.  2.  4.  6. -1.  0.]\n",
      " [ 0.  9.  7.  5. -3.  0.]\n",
      " [ 0. -1. -2. -3. -4.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n",
      "dL_dX\n",
      "[[ 17.  18.  14.  11.]\n",
      " [ 18.  17.   3.   7.]\n",
      " [ 24.  26.  -4.   0.]\n",
      " [  2. -13. -23. -12.]]\n",
      "updated_features\n",
      "[[ 1.52 -1.22 -0.59]\n",
      " [ 0.97 -0.84 -1.48]\n",
      " [-0.47 -1.47 -3.48]]\n"
     ]
    }
   ],
   "source": [
    "# # tests 3\n",
    "# # def back_prop(self, img_in, loss, features, learning_rate)\n",
    "# def test_conv_backward():\n",
    "#     test_arr = np.array(([5, 5, 2, 7],\n",
    "#                          [5, 5, 5, 0],\n",
    "#                          [5, 5, 5, 22],\n",
    "#                          [4, 3, 2, 1]))\n",
    "#     feat_arr = np.array(([2, -1, 0], \n",
    "#                          [2, 1, 1],\n",
    "#                          [1, 0, -1]))\n",
    "#     loss = np.array(([5, 3, 1, 9], \n",
    "#                      [2, 4, 6, -1],\n",
    "#                      [9, 7, 5, -3],\n",
    "#                      [-1,-2,-3,-4]))\n",
    "#     conv_layer = Convolution(1, 3, 1)\n",
    "#     conv_layer.back_prop(test_arr, loss, feat_arr, 0.01, 1)\n",
    "    \n",
    "# print(\"TEST: conv_back_prop\")\n",
    "# test_conv_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: pool_back_prop\n",
      "(array([[ 6.,  8.],\n",
      "       [ 5.,  4.]]), array([[ 0.,  0.,  0.,  1.],\n",
      "       [ 0.,  1.,  0.,  0.],\n",
      "       [ 1.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.]]))\n",
      "[[ 0.  0.  0.  9.]\n",
      " [ 0.  4.  0.  0.]\n",
      " [ 1.  0. -8.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# # def back_prop(self, loss):\n",
    "# def test_pooling_backward():\n",
    "#     test_arr = np.array(([5, 5, 2, 8],\n",
    "#                              [5, 6, 2, 1],\n",
    "#                              [5, 5, 4, -4],\n",
    "#                              [1, 4, 2, 3]))\n",
    "#     test_loss = np.array(([9,4],[1,-8]))\n",
    "#     pool_dim = 2\n",
    "#     pool_layer = Pooling(pool_dim)\n",
    "#     print(pool_layer.forward_prop(test_arr))\n",
    "#     print(pool_layer.back_prop(test_loss))\n",
    "    \n",
    "# print(\"TEST: pool_back_prop\")\n",
    "# test_pooling_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: relu_back_prop\n",
      "back_loss\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  5.  6.]\n",
      " [ 7.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# # def back_prop(self, loss):\n",
    "# def test_ReLU_backward():\n",
    "#     test_arr = np.array(([5, -1, -1],\n",
    "#                          [-4, 6, 2],\n",
    "#                          [5, 0, -3333]))\n",
    "#     ReLU_layer = ReLU()\n",
    "#     ReLU_layer.forward_prop(test_arr)\n",
    "#     test_loss = np.array(([1, 2, 3],\n",
    "#                          [4, 5, 6],\n",
    "#                          [7, 8, 9]))\n",
    "#     ReLU_layer.back_prop(test_loss)\n",
    "\n",
    "# print(\"TEST: relu_back_prop\")\n",
    "# test_ReLU_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
